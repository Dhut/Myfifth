{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EP_S2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhut/Myfifth/blob/master/Notebook/EP_S2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOp_Zq1S509W",
        "colab_type": "code",
        "outputId": "fd75c630-a60d-4b9c-f317-ae352d7b3090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import np_utils, to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "##for text cleaning\n",
        "import re, string"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2Z8hlgOd9Ql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52ead0a4-ceda-4652-f49f-c4db8ce94755"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aej9ALTtZ6Zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls /content/drive/'My Drive'/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPVfEXDJ51tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"Wonderland.txt\"\n",
        "raw_text = open(\"/content/drive/My Drive/\" + filename).read()\n",
        "raw_text = raw_text.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSjtG9B-51wC",
        "colab_type": "code",
        "outputId": "98c6ee30-e408-4287-9813-eab31ac230ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "print(raw_text[:500])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿project gutenberg's alice's adventures in wonderland, by lewis carroll\n",
            "\n",
            "this ebook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  you may copy it, give it away or\n",
            "re-use it under the terms of the project gutenberg license included\n",
            "with this ebook or online at www.gutenberg.org\n",
            "\n",
            "\n",
            "title: alice's adventures in wonderland\n",
            "\n",
            "author: lewis carroll\n",
            "\n",
            "posting date: june 25, 2008 [ebook #11]\n",
            "release date: march, 1994\n",
            "[last updated: december 20, 2011]\n",
            "\n",
            "language: e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5m0ZDgtoIEk",
        "colab_type": "text"
      },
      "source": [
        "# Remove Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbK1bIRjn0ve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "f79205fb-2c74-4341-98b7-4e20e09d2274"
      },
      "source": [
        "raw_text = re.sub('[%s]' % re.escape(string.punctuation), '', raw_text)\n",
        "print(raw_text[:500])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿project gutenbergs alices adventures in wonderland by lewis carroll\n",
            "\n",
            "this ebook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever  you may copy it give it away or\n",
            "reuse it under the terms of the project gutenberg license included\n",
            "with this ebook or online at wwwgutenbergorg\n",
            "\n",
            "\n",
            "title alices adventures in wonderland\n",
            "\n",
            "author lewis carroll\n",
            "\n",
            "posting date june 25 2008 ebook 11\n",
            "release date march 1994\n",
            "last updated december 20 2011\n",
            "\n",
            "language english\n",
            "\n",
            "\n",
            " start of this\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XdehaxIsR2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d083747c-bc83-4616-c47d-c776322921cc"
      },
      "source": [
        "# create mapping of unique chars to integers, and a reverse mapping\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print (\"Total Characters:\", n_chars)\n",
        "print (\"Total Vocab: \", n_vocab)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Characters: 154861\n",
            "Total Vocab:  39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M7piG4xscYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "89a189f5-1144-45c2-ed31-e223bdc99136"
      },
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Patterns:  154761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqXu4ffoscle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7feDFumTsrhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "8a88dce0-ace8-435c-951b-182b291681d4"
      },
      "source": [
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 16:08:41.630527 140636580714368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0721 16:08:41.646975 140636580714368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 16:08:41.650741 140636580714368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0721 16:08:41.995847 140636580714368 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0721 16:08:42.003371 140636580714368 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUqmUUyaxPMD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "25a93628-71c3-4d1a-ee2e-9f163a7b9aa1"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0718 14:26:40.301729 139818388506496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0718 14:26:40.337764 139818388506496 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTKiLqeFsrtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwMWNDazs73v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5891eff-e0f3-4b43-d8b5-29488e62e791"
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=100, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "154761/154761 [==============================] - 487s 3ms/step - loss: 2.7189\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.71888, saving model to weights-improvement-01-2.7189.hdf5\n",
            "Epoch 2/100\n",
            "154761/154761 [==============================] - 486s 3ms/step - loss: 2.3931\n",
            "\n",
            "Epoch 00002: loss improved from 2.71888 to 2.39310, saving model to weights-improvement-02-2.3931.hdf5\n",
            "Epoch 3/100\n",
            "154761/154761 [==============================] - 488s 3ms/step - loss: 2.1796\n",
            "\n",
            "Epoch 00003: loss improved from 2.39310 to 2.17956, saving model to weights-improvement-03-2.1796.hdf5\n",
            "Epoch 4/100\n",
            "154761/154761 [==============================] - 486s 3ms/step - loss: 2.0354\n",
            "\n",
            "Epoch 00004: loss improved from 2.17956 to 2.03540, saving model to weights-improvement-04-2.0354.hdf5\n",
            "Epoch 5/100\n",
            "154761/154761 [==============================] - 487s 3ms/step - loss: 1.9327\n",
            "\n",
            "Epoch 00005: loss improved from 2.03540 to 1.93268, saving model to weights-improvement-05-1.9327.hdf5\n",
            "Epoch 6/100\n",
            "154761/154761 [==============================] - 486s 3ms/step - loss: 1.8511\n",
            "\n",
            "Epoch 00006: loss improved from 1.93268 to 1.85115, saving model to weights-improvement-06-1.8511.hdf5\n",
            "Epoch 7/100\n",
            "154761/154761 [==============================] - 485s 3ms/step - loss: 1.7828\n",
            "\n",
            "Epoch 00007: loss improved from 1.85115 to 1.78276, saving model to weights-improvement-07-1.7828.hdf5\n",
            "Epoch 8/100\n",
            "154761/154761 [==============================] - 484s 3ms/step - loss: 1.7259\n",
            "\n",
            "Epoch 00008: loss improved from 1.78276 to 1.72587, saving model to weights-improvement-08-1.7259.hdf5\n",
            "Epoch 9/100\n",
            "154761/154761 [==============================] - 485s 3ms/step - loss: 1.6738\n",
            "\n",
            "Epoch 00009: loss improved from 1.72587 to 1.67380, saving model to weights-improvement-09-1.6738.hdf5\n",
            "Epoch 10/100\n",
            "154761/154761 [==============================] - 485s 3ms/step - loss: 1.6280\n",
            "\n",
            "Epoch 00010: loss improved from 1.67380 to 1.62803, saving model to weights-improvement-10-1.6280.hdf5\n",
            "Epoch 11/100\n",
            "154761/154761 [==============================] - 486s 3ms/step - loss: 1.5830\n",
            "\n",
            "Epoch 00011: loss improved from 1.62803 to 1.58299, saving model to weights-improvement-11-1.5830.hdf5\n",
            "Epoch 12/100\n",
            "154761/154761 [==============================] - 483s 3ms/step - loss: 1.5484\n",
            "\n",
            "Epoch 00012: loss improved from 1.58299 to 1.54836, saving model to weights-improvement-12-1.5484.hdf5\n",
            "Epoch 13/100\n",
            "154761/154761 [==============================] - 484s 3ms/step - loss: 1.5118\n",
            "\n",
            "Epoch 00013: loss improved from 1.54836 to 1.51181, saving model to weights-improvement-13-1.5118.hdf5\n",
            "Epoch 14/100\n",
            "154761/154761 [==============================] - 483s 3ms/step - loss: 1.4769\n",
            "\n",
            "Epoch 00014: loss improved from 1.51181 to 1.47687, saving model to weights-improvement-14-1.4769.hdf5\n",
            "Epoch 15/100\n",
            "154761/154761 [==============================] - 485s 3ms/step - loss: 1.4430\n",
            "\n",
            "Epoch 00015: loss improved from 1.47687 to 1.44303, saving model to weights-improvement-15-1.4430.hdf5\n",
            "Epoch 16/100\n",
            "154761/154761 [==============================] - 485s 3ms/step - loss: 1.4166\n",
            "\n",
            "Epoch 00016: loss improved from 1.44303 to 1.41661, saving model to weights-improvement-16-1.4166.hdf5\n",
            "Epoch 17/100\n",
            "154761/154761 [==============================] - 484s 3ms/step - loss: 1.3893\n",
            "\n",
            "Epoch 00017: loss improved from 1.41661 to 1.38930, saving model to weights-improvement-17-1.3893.hdf5\n",
            "Epoch 18/100\n",
            "154761/154761 [==============================] - 484s 3ms/step - loss: 1.3618\n",
            "\n",
            "Epoch 00018: loss improved from 1.38930 to 1.36182, saving model to weights-improvement-18-1.3618.hdf5\n",
            "Epoch 19/100\n",
            "154761/154761 [==============================] - 483s 3ms/step - loss: 1.3370\n",
            "\n",
            "Epoch 00019: loss improved from 1.36182 to 1.33696, saving model to weights-improvement-19-1.3370.hdf5\n",
            "Epoch 20/100\n",
            "154761/154761 [==============================] - 485s 3ms/step - loss: 1.3126\n",
            "\n",
            "Epoch 00020: loss improved from 1.33696 to 1.31255, saving model to weights-improvement-20-1.3126.hdf5\n",
            "Epoch 21/100\n",
            "154761/154761 [==============================] - 484s 3ms/step - loss: 1.2881\n",
            "\n",
            "Epoch 00021: loss improved from 1.31255 to 1.28807, saving model to weights-improvement-21-1.2881.hdf5\n",
            "Epoch 22/100\n",
            "154761/154761 [==============================] - 486s 3ms/step - loss: 1.2670\n",
            "\n",
            "Epoch 00022: loss improved from 1.28807 to 1.26701, saving model to weights-improvement-22-1.2670.hdf5\n",
            "Epoch 23/100\n",
            "154761/154761 [==============================] - 489s 3ms/step - loss: 1.2461\n",
            "\n",
            "Epoch 00023: loss improved from 1.26701 to 1.24613, saving model to weights-improvement-23-1.2461.hdf5\n",
            "Epoch 24/100\n",
            "154761/154761 [==============================] - 488s 3ms/step - loss: 1.2258\n",
            "\n",
            "Epoch 00024: loss improved from 1.24613 to 1.22580, saving model to weights-improvement-24-1.2258.hdf5\n",
            "Epoch 25/100\n",
            "154761/154761 [==============================] - 489s 3ms/step - loss: 1.2060\n",
            "\n",
            "Epoch 00025: loss improved from 1.22580 to 1.20601, saving model to weights-improvement-25-1.2060.hdf5\n",
            "Epoch 26/100\n",
            "154761/154761 [==============================] - 487s 3ms/step - loss: 1.1838\n",
            "\n",
            "Epoch 00026: loss improved from 1.20601 to 1.18377, saving model to weights-improvement-26-1.1838.hdf5\n",
            "Epoch 27/100\n",
            "154761/154761 [==============================] - 486s 3ms/step - loss: 1.1664\n",
            "\n",
            "Epoch 00027: loss improved from 1.18377 to 1.16639, saving model to weights-improvement-27-1.1664.hdf5\n",
            "Epoch 28/100\n",
            "154761/154761 [==============================] - 487s 3ms/step - loss: 1.1477\n",
            "\n",
            "Epoch 00028: loss improved from 1.16639 to 1.14773, saving model to weights-improvement-28-1.1477.hdf5\n",
            "Epoch 29/100\n",
            "154761/154761 [==============================] - 485s 3ms/step - loss: 1.1289\n",
            "\n",
            "Epoch 00029: loss improved from 1.14773 to 1.12892, saving model to weights-improvement-29-1.1289.hdf5\n",
            "Epoch 30/100\n",
            "154761/154761 [==============================] - 485s 3ms/step - loss: 1.1123\n",
            "\n",
            "Epoch 00030: loss improved from 1.12892 to 1.11226, saving model to weights-improvement-30-1.1123.hdf5\n",
            "Epoch 31/100\n",
            "154761/154761 [==============================] - 485s 3ms/step - loss: 1.0983\n",
            "\n",
            "Epoch 00031: loss improved from 1.11226 to 1.09827, saving model to weights-improvement-31-1.0983.hdf5\n",
            "Epoch 32/100\n",
            "154761/154761 [==============================] - 486s 3ms/step - loss: 1.0830\n",
            "\n",
            "Epoch 00032: loss improved from 1.09827 to 1.08296, saving model to weights-improvement-32-1.0830.hdf5\n",
            "Epoch 33/100\n",
            "154761/154761 [==============================] - 483s 3ms/step - loss: 1.0652\n",
            "\n",
            "Epoch 00033: loss improved from 1.08296 to 1.06519, saving model to weights-improvement-33-1.0652.hdf5\n",
            "Epoch 34/100\n",
            "154761/154761 [==============================] - 483s 3ms/step - loss: 1.0553\n",
            "\n",
            "Epoch 00034: loss improved from 1.06519 to 1.05527, saving model to weights-improvement-34-1.0553.hdf5\n",
            "Epoch 35/100\n",
            "154761/154761 [==============================] - 487s 3ms/step - loss: 1.0416\n",
            "\n",
            "Epoch 00035: loss improved from 1.05527 to 1.04161, saving model to weights-improvement-35-1.0416.hdf5\n",
            "Epoch 36/100\n",
            "154761/154761 [==============================] - 489s 3ms/step - loss: 1.0256\n",
            "\n",
            "Epoch 00036: loss improved from 1.04161 to 1.02560, saving model to weights-improvement-36-1.0256.hdf5\n",
            "Epoch 37/100\n",
            "154761/154761 [==============================] - 488s 3ms/step - loss: 1.0114\n",
            "\n",
            "Epoch 00037: loss improved from 1.02560 to 1.01143, saving model to weights-improvement-37-1.0114.hdf5\n",
            "Epoch 38/100\n",
            "154761/154761 [==============================] - 487s 3ms/step - loss: 1.0004\n",
            "\n",
            "Epoch 00038: loss improved from 1.01143 to 1.00044, saving model to weights-improvement-38-1.0004.hdf5\n",
            "Epoch 39/100\n",
            "154761/154761 [==============================] - 488s 3ms/step - loss: 0.9856\n",
            "\n",
            "Epoch 00039: loss improved from 1.00044 to 0.98563, saving model to weights-improvement-39-0.9856.hdf5\n",
            "Epoch 40/100\n",
            "154761/154761 [==============================] - 488s 3ms/step - loss: 0.9816\n",
            "\n",
            "Epoch 00040: loss improved from 0.98563 to 0.98161, saving model to weights-improvement-40-0.9816.hdf5\n",
            "Epoch 41/100\n",
            "154761/154761 [==============================] - 478s 3ms/step - loss: 0.9639\n",
            "\n",
            "Epoch 00041: loss improved from 0.98161 to 0.96385, saving model to weights-improvement-41-0.9639.hdf5\n",
            "Epoch 42/100\n",
            "154761/154761 [==============================] - 478s 3ms/step - loss: 0.9563\n",
            "\n",
            "Epoch 00042: loss improved from 0.96385 to 0.95625, saving model to weights-improvement-42-0.9563.hdf5\n",
            "Epoch 43/100\n",
            "154761/154761 [==============================] - 477s 3ms/step - loss: 0.9416\n",
            "\n",
            "Epoch 00043: loss improved from 0.95625 to 0.94163, saving model to weights-improvement-43-0.9416.hdf5\n",
            "Epoch 44/100\n",
            "154761/154761 [==============================] - 475s 3ms/step - loss: 0.9334\n",
            "\n",
            "Epoch 00044: loss improved from 0.94163 to 0.93340, saving model to weights-improvement-44-0.9334.hdf5\n",
            "Epoch 45/100\n",
            "154761/154761 [==============================] - 473s 3ms/step - loss: 0.9202\n",
            "\n",
            "Epoch 00045: loss improved from 0.93340 to 0.92022, saving model to weights-improvement-45-0.9202.hdf5\n",
            "Epoch 46/100\n",
            "154761/154761 [==============================] - 473s 3ms/step - loss: 0.9120\n",
            "\n",
            "Epoch 00046: loss improved from 0.92022 to 0.91199, saving model to weights-improvement-46-0.9120.hdf5\n",
            "Epoch 47/100\n",
            "154761/154761 [==============================] - 474s 3ms/step - loss: 0.9058\n",
            "\n",
            "Epoch 00047: loss improved from 0.91199 to 0.90583, saving model to weights-improvement-47-0.9058.hdf5\n",
            "Epoch 48/100\n",
            "154761/154761 [==============================] - 470s 3ms/step - loss: 0.8935\n",
            "\n",
            "Epoch 00048: loss improved from 0.90583 to 0.89346, saving model to weights-improvement-48-0.8935.hdf5\n",
            "Epoch 49/100\n",
            "154761/154761 [==============================] - 475s 3ms/step - loss: 0.8831\n",
            "\n",
            "Epoch 00049: loss improved from 0.89346 to 0.88307, saving model to weights-improvement-49-0.8831.hdf5\n",
            "Epoch 50/100\n",
            "154761/154761 [==============================] - 475s 3ms/step - loss: 0.8747\n",
            "\n",
            "Epoch 00050: loss improved from 0.88307 to 0.87472, saving model to weights-improvement-50-0.8747.hdf5\n",
            "Epoch 51/100\n",
            "154761/154761 [==============================] - 474s 3ms/step - loss: 0.8714\n",
            "\n",
            "Epoch 00051: loss improved from 0.87472 to 0.87140, saving model to weights-improvement-51-0.8714.hdf5\n",
            "Epoch 52/100\n",
            "154761/154761 [==============================] - 470s 3ms/step - loss: 0.8589\n",
            "\n",
            "Epoch 00052: loss improved from 0.87140 to 0.85888, saving model to weights-improvement-52-0.8589.hdf5\n",
            "Epoch 53/100\n",
            "154761/154761 [==============================] - 473s 3ms/step - loss: 0.8495\n",
            "\n",
            "Epoch 00053: loss improved from 0.85888 to 0.84949, saving model to weights-improvement-53-0.8495.hdf5\n",
            "Epoch 54/100\n",
            "154761/154761 [==============================] - 472s 3ms/step - loss: 0.8420\n",
            "\n",
            "Epoch 00054: loss improved from 0.84949 to 0.84205, saving model to weights-improvement-54-0.8420.hdf5\n",
            "Epoch 55/100\n",
            "154761/154761 [==============================] - 462s 3ms/step - loss: 0.8384\n",
            "\n",
            "Epoch 00055: loss improved from 0.84205 to 0.83836, saving model to weights-improvement-55-0.8384.hdf5\n",
            "Epoch 56/100\n",
            "154761/154761 [==============================] - 461s 3ms/step - loss: 0.8292\n",
            "\n",
            "Epoch 00056: loss improved from 0.83836 to 0.82923, saving model to weights-improvement-56-0.8292.hdf5\n",
            "Epoch 57/100\n",
            "154761/154761 [==============================] - 468s 3ms/step - loss: 0.8144\n",
            "\n",
            "Epoch 00057: loss improved from 0.82923 to 0.81442, saving model to weights-improvement-57-0.8144.hdf5\n",
            "Epoch 58/100\n",
            "154761/154761 [==============================] - 466s 3ms/step - loss: 0.8216\n",
            "\n",
            "Epoch 00058: loss did not improve from 0.81442\n",
            "Epoch 59/100\n",
            "154761/154761 [==============================] - 463s 3ms/step - loss: 0.8106\n",
            "\n",
            "Epoch 00059: loss improved from 0.81442 to 0.81059, saving model to weights-improvement-59-0.8106.hdf5\n",
            "Epoch 60/100\n",
            "154761/154761 [==============================] - 458s 3ms/step - loss: 0.7966\n",
            "\n",
            "Epoch 00060: loss improved from 0.81059 to 0.79657, saving model to weights-improvement-60-0.7966.hdf5\n",
            "Epoch 61/100\n",
            "154761/154761 [==============================] - 461s 3ms/step - loss: 0.7916\n",
            "\n",
            "Epoch 00061: loss improved from 0.79657 to 0.79156, saving model to weights-improvement-61-0.7916.hdf5\n",
            "Epoch 62/100\n",
            "154761/154761 [==============================] - 463s 3ms/step - loss: 0.7860\n",
            "\n",
            "Epoch 00062: loss improved from 0.79156 to 0.78603, saving model to weights-improvement-62-0.7860.hdf5\n",
            "Epoch 63/100\n",
            "154761/154761 [==============================] - 456s 3ms/step - loss: 0.7781\n",
            "\n",
            "Epoch 00063: loss improved from 0.78603 to 0.77814, saving model to weights-improvement-63-0.7781.hdf5\n",
            "Epoch 64/100\n",
            "154761/154761 [==============================] - 454s 3ms/step - loss: 0.7800\n",
            "\n",
            "Epoch 00064: loss did not improve from 0.77814\n",
            "Epoch 65/100\n",
            "154761/154761 [==============================] - 453s 3ms/step - loss: 0.7706\n",
            "\n",
            "Epoch 00065: loss improved from 0.77814 to 0.77057, saving model to weights-improvement-65-0.7706.hdf5\n",
            "Epoch 66/100\n",
            "154761/154761 [==============================] - 450s 3ms/step - loss: 0.7612\n",
            "\n",
            "Epoch 00066: loss improved from 0.77057 to 0.76122, saving model to weights-improvement-66-0.7612.hdf5\n",
            "Epoch 67/100\n",
            "154761/154761 [==============================] - 450s 3ms/step - loss: 0.7567\n",
            "\n",
            "Epoch 00067: loss improved from 0.76122 to 0.75668, saving model to weights-improvement-67-0.7567.hdf5\n",
            "Epoch 68/100\n",
            "154761/154761 [==============================] - 450s 3ms/step - loss: 0.7473\n",
            "\n",
            "Epoch 00068: loss improved from 0.75668 to 0.74731, saving model to weights-improvement-68-0.7473.hdf5\n",
            "Epoch 69/100\n",
            "154761/154761 [==============================] - 450s 3ms/step - loss: 0.7439\n",
            "\n",
            "Epoch 00069: loss improved from 0.74731 to 0.74386, saving model to weights-improvement-69-0.7439.hdf5\n",
            "Epoch 70/100\n",
            "154761/154761 [==============================] - 448s 3ms/step - loss: 0.7378\n",
            "\n",
            "Epoch 00070: loss improved from 0.74386 to 0.73784, saving model to weights-improvement-70-0.7378.hdf5\n",
            "Epoch 71/100\n",
            "154761/154761 [==============================] - 448s 3ms/step - loss: 1.0374\n",
            "\n",
            "Epoch 00071: loss did not improve from 0.73784\n",
            "Epoch 72/100\n",
            "154761/154761 [==============================] - 444s 3ms/step - loss: 0.9621\n",
            "\n",
            "Epoch 00072: loss did not improve from 0.73784\n",
            "Epoch 73/100\n",
            "154761/154761 [==============================] - 440s 3ms/step - loss: 0.8736\n",
            "\n",
            "Epoch 00073: loss did not improve from 0.73784\n",
            "Epoch 74/100\n",
            "154761/154761 [==============================] - 439s 3ms/step - loss: 0.8099\n",
            "\n",
            "Epoch 00074: loss did not improve from 0.73784\n",
            "Epoch 75/100\n",
            "154761/154761 [==============================] - 438s 3ms/step - loss: 0.7440\n",
            "\n",
            "Epoch 00075: loss did not improve from 0.73784\n",
            "Epoch 76/100\n",
            "154761/154761 [==============================] - 433s 3ms/step - loss: 0.7651\n",
            "\n",
            "Epoch 00076: loss did not improve from 0.73784\n",
            "Epoch 77/100\n",
            "154761/154761 [==============================] - 433s 3ms/step - loss: 0.7153\n",
            "\n",
            "Epoch 00077: loss improved from 0.73784 to 0.71535, saving model to weights-improvement-77-0.7153.hdf5\n",
            "Epoch 78/100\n",
            "154761/154761 [==============================] - 432s 3ms/step - loss: 0.7099\n",
            "\n",
            "Epoch 00078: loss improved from 0.71535 to 0.70989, saving model to weights-improvement-78-0.7099.hdf5\n",
            "Epoch 79/100\n",
            "154761/154761 [==============================] - 432s 3ms/step - loss: 0.7083\n",
            "\n",
            "Epoch 00079: loss improved from 0.70989 to 0.70831, saving model to weights-improvement-79-0.7083.hdf5\n",
            "Epoch 80/100\n",
            " 42880/154761 [=======>......................] - ETA: 5:11 - loss: 1.2903"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll_VVAaIs8BU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTORDxpxs7-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# load the network weights\n",
        "filename = \"/content/drive/My Drive/weights-improvement-63-0.7781.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv8gVh65s76y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd5b4435-bb24-432e-d5ad-fc489123bcc8"
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=37, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0718 14:34:53.127633 139818388506496 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/37\n",
            "154761/154761 [==============================] - 454s 3ms/step - loss: 0.7766\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.77657, saving model to weights-improvement-01-0.7766.hdf5\n",
            "Epoch 2/37\n",
            "154761/154761 [==============================] - 453s 3ms/step - loss: 0.7608\n",
            "\n",
            "Epoch 00002: loss improved from 0.77657 to 0.76079, saving model to weights-improvement-02-0.7608.hdf5\n",
            "Epoch 3/37\n",
            "154761/154761 [==============================] - 452s 3ms/step - loss: 0.7553\n",
            "\n",
            "Epoch 00003: loss improved from 0.76079 to 0.75532, saving model to weights-improvement-03-0.7553.hdf5\n",
            "Epoch 4/37\n",
            "154761/154761 [==============================] - 451s 3ms/step - loss: 0.7610\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.75532\n",
            "Epoch 5/37\n",
            "154761/154761 [==============================] - 449s 3ms/step - loss: 0.7486\n",
            "\n",
            "Epoch 00005: loss improved from 0.75532 to 0.74865, saving model to weights-improvement-05-0.7486.hdf5\n",
            "Epoch 6/37\n",
            "154761/154761 [==============================] - 450s 3ms/step - loss: 0.7416\n",
            "\n",
            "Epoch 00006: loss improved from 0.74865 to 0.74158, saving model to weights-improvement-06-0.7416.hdf5\n",
            "Epoch 7/37\n",
            "154761/154761 [==============================] - 450s 3ms/step - loss: 0.7396\n",
            "\n",
            "Epoch 00007: loss improved from 0.74158 to 0.73963, saving model to weights-improvement-07-0.7396.hdf5\n",
            "Epoch 8/37\n",
            "154761/154761 [==============================] - 452s 3ms/step - loss: 0.7376\n",
            "\n",
            "Epoch 00008: loss improved from 0.73963 to 0.73764, saving model to weights-improvement-08-0.7376.hdf5\n",
            "Epoch 9/37\n",
            "154761/154761 [==============================] - 450s 3ms/step - loss: 0.7259\n",
            "\n",
            "Epoch 00009: loss improved from 0.73764 to 0.72595, saving model to weights-improvement-09-0.7259.hdf5\n",
            "Epoch 10/37\n",
            "154761/154761 [==============================] - 451s 3ms/step - loss: 0.7191\n",
            "\n",
            "Epoch 00010: loss improved from 0.72595 to 0.71907, saving model to weights-improvement-10-0.7191.hdf5\n",
            "Epoch 11/37\n",
            "154761/154761 [==============================] - 452s 3ms/step - loss: 0.7151\n",
            "\n",
            "Epoch 00011: loss improved from 0.71907 to 0.71511, saving model to weights-improvement-11-0.7151.hdf5\n",
            "Epoch 12/37\n",
            "154761/154761 [==============================] - 451s 3ms/step - loss: 0.7079\n",
            "\n",
            "Epoch 00012: loss improved from 0.71511 to 0.70791, saving model to weights-improvement-12-0.7079.hdf5\n",
            "Epoch 13/37\n",
            "154761/154761 [==============================] - 449s 3ms/step - loss: 0.7079\n",
            "\n",
            "Epoch 00013: loss did not improve from 0.70791\n",
            "Epoch 14/37\n",
            "154761/154761 [==============================] - 456s 3ms/step - loss: 0.6968\n",
            "\n",
            "Epoch 00014: loss improved from 0.70791 to 0.69681, saving model to weights-improvement-14-0.6968.hdf5\n",
            "Epoch 15/37\n",
            "154761/154761 [==============================] - 454s 3ms/step - loss: 0.7000\n",
            "\n",
            "Epoch 00015: loss did not improve from 0.69681\n",
            "Epoch 16/37\n",
            "154761/154761 [==============================] - 452s 3ms/step - loss: 0.6955\n",
            "\n",
            "Epoch 00016: loss improved from 0.69681 to 0.69546, saving model to weights-improvement-16-0.6955.hdf5\n",
            "Epoch 17/37\n",
            "154761/154761 [==============================] - 453s 3ms/step - loss: 0.6896\n",
            "\n",
            "Epoch 00017: loss improved from 0.69546 to 0.68956, saving model to weights-improvement-17-0.6896.hdf5\n",
            "Epoch 18/37\n",
            "154761/154761 [==============================] - 454s 3ms/step - loss: 0.6784\n",
            "\n",
            "Epoch 00018: loss improved from 0.68956 to 0.67839, saving model to weights-improvement-18-0.6784.hdf5\n",
            "Epoch 19/37\n",
            "154761/154761 [==============================] - 452s 3ms/step - loss: 0.6735\n",
            "\n",
            "Epoch 00019: loss improved from 0.67839 to 0.67349, saving model to weights-improvement-19-0.6735.hdf5\n",
            "Epoch 20/37\n",
            "154761/154761 [==============================] - 452s 3ms/step - loss: 0.6738\n",
            "\n",
            "Epoch 00020: loss did not improve from 0.67349\n",
            "Epoch 21/37\n",
            "154761/154761 [==============================] - 451s 3ms/step - loss: 0.6677\n",
            "\n",
            "Epoch 00021: loss improved from 0.67349 to 0.66765, saving model to weights-improvement-21-0.6677.hdf5\n",
            "Epoch 22/37\n",
            "154761/154761 [==============================] - 448s 3ms/step - loss: 0.6655\n",
            "\n",
            "Epoch 00022: loss improved from 0.66765 to 0.66550, saving model to weights-improvement-22-0.6655.hdf5\n",
            "Epoch 23/37\n",
            "154761/154761 [==============================] - 454s 3ms/step - loss: 0.6574\n",
            "\n",
            "Epoch 00023: loss improved from 0.66550 to 0.65744, saving model to weights-improvement-23-0.6574.hdf5\n",
            "Epoch 24/37\n",
            "154761/154761 [==============================] - 453s 3ms/step - loss: 0.6606\n",
            "\n",
            "Epoch 00024: loss did not improve from 0.65744\n",
            "Epoch 25/37\n",
            "154761/154761 [==============================] - 453s 3ms/step - loss: 0.6511\n",
            "\n",
            "Epoch 00025: loss improved from 0.65744 to 0.65109, saving model to weights-improvement-25-0.6511.hdf5\n",
            "Epoch 26/37\n",
            "154761/154761 [==============================] - 459s 3ms/step - loss: 0.6498\n",
            "\n",
            "Epoch 00026: loss improved from 0.65109 to 0.64984, saving model to weights-improvement-26-0.6498.hdf5\n",
            "Epoch 27/37\n",
            "154761/154761 [==============================] - 455s 3ms/step - loss: 0.6381\n",
            "\n",
            "Epoch 00027: loss improved from 0.64984 to 0.63812, saving model to weights-improvement-27-0.6381.hdf5\n",
            "Epoch 28/37\n",
            "154761/154761 [==============================] - 455s 3ms/step - loss: 0.6435\n",
            "\n",
            "Epoch 00028: loss did not improve from 0.63812\n",
            "Epoch 29/37\n",
            "154761/154761 [==============================] - 459s 3ms/step - loss: 0.6332\n",
            "\n",
            "Epoch 00029: loss improved from 0.63812 to 0.63315, saving model to weights-improvement-29-0.6332.hdf5\n",
            "Epoch 30/37\n",
            "154761/154761 [==============================] - 456s 3ms/step - loss: 0.6330\n",
            "\n",
            "Epoch 00030: loss improved from 0.63315 to 0.63297, saving model to weights-improvement-30-0.6330.hdf5\n",
            "Epoch 31/37\n",
            "154761/154761 [==============================] - 455s 3ms/step - loss: 0.6211\n",
            "\n",
            "Epoch 00031: loss improved from 0.63297 to 0.62115, saving model to weights-improvement-31-0.6211.hdf5\n",
            "Epoch 32/37\n",
            "154761/154761 [==============================] - 452s 3ms/step - loss: 0.6252\n",
            "\n",
            "Epoch 00032: loss did not improve from 0.62115\n",
            "Epoch 33/37\n",
            "154761/154761 [==============================] - 453s 3ms/step - loss: 0.6206\n",
            "\n",
            "Epoch 00033: loss improved from 0.62115 to 0.62055, saving model to weights-improvement-33-0.6206.hdf5\n",
            "Epoch 34/37\n",
            "154761/154761 [==============================] - 453s 3ms/step - loss: 0.6220\n",
            "\n",
            "Epoch 00034: loss did not improve from 0.62055\n",
            "Epoch 35/37\n",
            "154761/154761 [==============================] - 456s 3ms/step - loss: 0.6167\n",
            "\n",
            "Epoch 00035: loss improved from 0.62055 to 0.61673, saving model to weights-improvement-35-0.6167.hdf5\n",
            "Epoch 36/37\n",
            "154761/154761 [==============================] - 456s 3ms/step - loss: 0.6080\n",
            "\n",
            "Epoch 00036: loss improved from 0.61673 to 0.60803, saving model to weights-improvement-36-0.6080.hdf5\n",
            "Epoch 37/37\n",
            "154761/154761 [==============================] - 457s 3ms/step - loss: 0.6091\n",
            "\n",
            "Epoch 00037: loss did not improve from 0.60803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f29b2829f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3A33X7lTL0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9RMHqJ2ZTMQY",
        "colab": {}
      },
      "source": [
        "\n",
        "# load the network weights\n",
        "filename = \"/content/drive/My Drive/weights-improvement-36-0.6080.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "51f23448-1604-482a-cbed-b1af05ae6c59",
        "id": "G1m_GF-HTMQi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "# fit the model\n",
        "model.fit(X, y, epochs=3, batch_size=128, callbacks=callbacks_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0721 16:12:22.870857 140636580714368 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "154761/154761 [==============================] - 325s 2ms/step - loss: 0.6132\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.61318, saving model to weights-improvement-01-0.6132.hdf5\n",
            "Epoch 2/3\n",
            "154761/154761 [==============================] - 321s 2ms/step - loss: 0.6008\n",
            "\n",
            "Epoch 00002: loss improved from 0.61318 to 0.60083, saving model to weights-improvement-02-0.6008.hdf5\n",
            "Epoch 3/3\n",
            "154761/154761 [==============================] - 320s 2ms/step - loss: 0.6003\n",
            "\n",
            "Epoch 00003: loss improved from 0.60083 to 0.60027, saving model to weights-improvement-03-0.6003.hdf5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe865ee7240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgg3F4oOhDNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkrKiPJVq2be",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "4e5e9c41-e4dd-4544-851f-79d0f67055f1"
      },
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(500):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" aging opening for a conversation alice replied\n",
            "rather shyly ii hardly know sir just at presentat lea \"\n",
            "st to do with anowher minute theres no harge you she said to herself im any antthing blice thought she had never like that \n",
            "alice was that all telse the puppy filled with a little gound ier beotalne so do so alice they seemed to be a little lork coneusion that he said to the gryphon \n",
            "you meyer said the queen \n",
            "i wish i hadnt cried the mouse with a long and then\n",
            "said together\n",
            "\n",
            "what for said the king and the little door and the loral of that isce out why yould bll walted to see it tro this time and\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1Br1t1oq2Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtfSaiWPq2XI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}